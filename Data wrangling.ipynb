{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import tweepy\n",
    "import os\n",
    "import seaborn as sns\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading dataset and printing first two rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive =  pd.read_csv('twitter-archive-enhanced.csv')\n",
    "df_twitter_archive.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering image prediction file on udacity hosted server "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using python request --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "response = requests.get(url)\n",
    "open('image_predictions.tsv', 'wb').write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_prediction = pd.read_csv('image_predictions.tsv',sep='\\t')\n",
    "df_image_prediction.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop running from here api call ahead will take 30-40 mins ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering twitter data throught tweepy : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = '5Uur0mo4ol2kB8yhtZ1VxXS0u'\n",
    "CONSUMER_SECRET = 'h8E7fSpXWiMoBel7G1ZOAeu4Mgru0v0MtxH5ehYE1RKM89SiBH'\n",
    "OAUTH_TOKEN = 'ct9aNnU0FQR0UKJVn1i1W3Y8omqSewiQWUcRaygB'\n",
    "OAUTH_TOKEN_SECRET ='D3qslrbdOU5fqTOp951kOIuZbkeTPBodnjNYoEGFR63Ft'\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "\n",
    "api = tweepy.API(auth, \n",
    "                 parser = tweepy.parsers.JSONParser(), \n",
    "                 wait_on_rate_limit = True, \n",
    "                 wait_on_rate_limit_notify = True)\n",
    "\n",
    "\n",
    "#Download Tweepy status object based on Tweet ID and store in list\n",
    "# Fetch tweets from the twitter API using the following loop:\n",
    "list_of_tweets = []\n",
    "# Tweets that can't be found are saved in the list below:\n",
    "cant_find_tweets_for_those_ids = []\n",
    "for each_id in my_ids:   \n",
    "    try:\n",
    "        list_of_tweets.append(api.get_status(each_id))\n",
    "    except Exception as e:\n",
    "        cant_find_tweets_for_those_ids.append(each_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data in list : \n",
    "\n",
    "print(\"The list of tweets\" ,len(list_of_tweets))\n",
    "print(\"The list of tweets no found\" , len(cant_find_tweets_for_those_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then in this code block we isolate the json part of each tweepy \n",
    "#status object that we have downloaded and we add them all into a list\n",
    "\n",
    "my_list_of_dicts = []\n",
    "for each_json_tweet in list_of_tweets:\n",
    "    my_list_of_dicts.append(each_json_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_json.txt', 'w') as file:\n",
    "        file.write(json.dumps(my_list_of_dicts, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify information of interest from JSON dictionaries in txt file\n",
    "#and put it in a dataframe called tweet JSON\n",
    "my_demo_list = []\n",
    "with open('tweet_json.txt',encoding ='utf-8') as json_file:\n",
    "    all_data = json.load(json_file)\n",
    "    for each_dictionary in all_data:\n",
    "        tweet_id = each_dictionary['id']\n",
    "        whole_tweet = each_dictionary['text']\n",
    "        only_url = whole_tweet[whole_tweet.find('https'):]\n",
    "        favorite_count = each_dictionary['favorite_count']\n",
    "        retweet_count = each_dictionary['retweet_count']\n",
    "        followers_count = each_dictionary['user']['followers_count']\n",
    "        friends_count = each_dictionary['user']['friends_count']\n",
    "        whole_source = each_dictionary['source']\n",
    "        only_device = whole_source[whole_source.find('rel=\"nofollow\">') + 15:-4]\n",
    "        source = only_device\n",
    "        retweeted_status = each_dictionary['retweeted_status'] = each_dictionary.get('retweeted_status', 'Original tweet')\n",
    "        if retweeted_status == 'Original tweet':\n",
    "            url = only_url\n",
    "        else:\n",
    "            retweeted_status = 'This is a retweet'\n",
    "            url = 'This is a retweet'\n",
    "        my_demo_list.append({\n",
    "            'tweet_id' : str(tweet_id),\n",
    "            'favorite_count' : int(favorite_count),\n",
    "            'retweet_count' : int(retweet_count),\n",
    "            'followers_count' : int(followers_count),\n",
    "            'friends_count' : int(friends_count),\n",
    "            'source' : source,\n",
    "            'retweeted_status' : retweeted_status,\n",
    "            'url' : url,\n",
    "            \n",
    "        })\n",
    "tweet_json = pd.DataFrame(my_demo_list, columns = ['tweet_id', 'favorite_count','retweet_count','followers_count', 'friends_count','source','retweeted_status', 'url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Running from here now : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json = pd.read_csv('tweet_json.csv')\n",
    "tweet_json.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving dataframe tweet_json to a csv file so that you dont have to run api call again when you rerun the code : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.to_csv('tweet_json.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assesing data for issues : \n",
    "#### Visual assesment / Programatic assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing tweet_json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_json.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(tweet_json.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assesing image prediction data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_prediction.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_prediction.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_prediction.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_prediction.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_image_prediction.jpg_url.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_prediction.p1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_prediction.p1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_prediction.img_num.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_prediction.p1_conf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_prediction.p2_conf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_image_prediction.tweet_id.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_prediction.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assesing twitter archive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_twitter_archive['tweet_id'].duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive['source'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive['source'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking all different values of rating numerator\n",
    "df_twitter_archive.rating_numerator.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking each value one by one for irregularities :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting some random values and inspecting them for reason of irregularitits\n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_numerator == 204, 'text']) \n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_numerator == 143, 'text']) \n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_numerator == 666, 'text']) \n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_numerator == 1176, 'text'])\n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_numerator == 144, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print whole text in order to verify numerators and denominators\n",
    "print(df_twitter_archive['text'][1120]) #17 dogs\n",
    "print(df_twitter_archive['text'][1634]) #13 dogs\n",
    "print(df_twitter_archive['text'][313]) #just a tweet to explain actual ratings, this will be ignored when cleaning data\n",
    "print(df_twitter_archive['text'][189]) #no picture, this will be ignored when cleaning data\n",
    "print(df_twitter_archive['text'][1779]) #12 dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking all different values of rating denominator\n",
    "\n",
    "df_twitter_archive.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 11, 'text'])\n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 2, 'text'])\n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 16, 'text'])\n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 110, 'text'])\n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 15, 'text'])\n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 90, 'text'])\n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 130, 'text'])\n",
    "print(df_twitter_archive.loc[df_twitter_archive.rating_denominator == 7, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.iloc[2335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_twitter_archive['text'][784]) # Retweet -- will be deleted\n",
    "print(df_twitter_archive['text'][1068]) # Actual rating 14/10 manual change\n",
    "print(df_twitter_archive['text'][1662]) # Actual rating 10/10 manual change\n",
    "print(df_twitter_archive['text'][2335]) # Actual rating 9/10 manual change\n",
    "print(df_twitter_archive['text'][1663]) # Tweet to explain rating\n",
    "print(df_twitter_archive['text'][1635]) # 11 dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by exploring (Reviewer point )i found that the data has some wrong numerators and denominators in rating that i will change manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('max_colwidth', 200):\n",
    "    display(df_twitter_archive[df_twitter_archive['text'].str.contains(r\"(\\d+\\.\\d*\\/\\d+)\")]\n",
    "            [['tweet_id', 'text', 'rating_numerator', 'rating_denominator']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here we can see that rating are wrong we will have to change them also manually "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning : \n",
    "    Quality : \n",
    "        df_twitter_archive : \n",
    "            1. Delete columns that wont be used for analysis(in_reply_to_status_id,in_reply_to_user_id,retweeted_status_id,retweeted_status_user_id,retweeted_status_timestamp,expanded_urls)\n",
    "            2. Separate timestamp into day - month - year (3 columns)\n",
    "            3. Delete retweets.\n",
    "            4. Correct numerators and denominators\n",
    "        df_image_prediction :\n",
    "            1. Drop 66 jpg_url duplicated\n",
    "            2. Create 1 column for image prediction and 1 column for confidence level\n",
    "            3. Delete columns that won't be used for analysis \n",
    "            \n",
    "        tweet_json :\n",
    "            1. Keep original tweets only\n",
    "            \n",
    "    Tidiness : \n",
    "        df_twitter_archive:\n",
    "            1. Melt dogoo puppo to one col dog statge\n",
    "            2. Change tweet_id to type int64 in order to merge with the other 2 tables\n",
    "            3. All tables should be part of one dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a copy of dataframes before cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean = df_twitter_archive.copy()\n",
    "image_prediction_clean = df_image_prediction.copy()\n",
    "tweet_json_clean = tweet_json.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning twitter_archive\n",
    "\n",
    "df_twitter_archive : \n",
    "        1. Delete columns that wont be used for analysis(in_reply_to_status_id,in_reply_to_user_id,retweeted_status_id,retweeted_status_user_id,retweeted_status_timestamp,expanded_urls)\n",
    "        2. Separate timestamp into day - month - year (3 columns)\n",
    "        3. Delete retweets.\n",
    "        4. Correct numerators and denominators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_archive.retweeted_status_id.value_counts().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Twitter archive - Delete retweets .\n",
    "\n",
    "Based on info, there are 181 values in retweeted_status_id and retweeted_status_user_id. Delete the retweets. Once I merge twitter_archive and image_prediction, I will only keep the ones with images.\n",
    "\n",
    "-- Keeping only original tweets deleting all retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean = twitter_archive_clean[pd.isnull(twitter_archive_clean['retweeted_status_user_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Twitter archive - Delete columns that won't be used for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: Delete columns no needed\n",
    "twitter_archive_clean.drop(['source','in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp','expanded_urls'],1,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(twitter_archive_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidiness : \n",
    "1. Twitter_archive - Melt (doggo, floofer, pupper and puppo columns) to one col dog statge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean = pd.melt(twitter_archive_clean,id_vars = ['tweet_id','timestamp','text','rating_numerator','rating_denominator','name'],var_name = 'dogs',value_name = 'dogs_stage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop dogs\n",
    "twitter_archive_clean = twitter_archive_clean.drop('dogs', 1)\n",
    "\n",
    "# Sort by dogs_stage then drop duplicated based on tweet_id except the last occurrence\n",
    "twitter_archive_clean = twitter_archive_clean.sort_values('dogs_stage').drop_duplicates(subset='tweet_id', \n",
    "                                                                                        keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test :\n",
    "twitter_archive_clean['dogs_stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_archive_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices are not ordered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = np.arange(twitter_archive_clean.shape[0])\n",
    "# twitter_archive_clean.index = indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Twitter_archive - Separate timestamp into day - month - year (3 columns)\n",
    "\n",
    "First convert timestamp to datetime. Then extract year, month and day to new columns. Finally drop timestamp column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp to datetime\n",
    "twitter_archive_clean.timestamp = pd.to_datetime(twitter_archive_clean.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract year, month and day to new columns\n",
    "\n",
    "twitter_archive_clean[\"day\"] = twitter_archive_clean['timestamp'].map(lambda x: x.day)\n",
    "twitter_archive_clean[\"month\"] = twitter_archive_clean['timestamp'].map(lambda x: x.month)\n",
    "twitter_archive_clean[\"year\"] = twitter_archive_clean['timestamp'].map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test :\n",
    "twitter_archive_clean.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the timestamp col not needed anymore\n",
    "twitter_archive_clean.drop('timestamp',1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "twitter_archive_clean.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Twitter_archive - Correc numerators \n",
    "\n",
    "Steps : \n",
    "1. first we will correct numerator that have wrong rating (wrong data extracted)\n",
    "2. then we will correct dog rating in which there are multiple dogs and that are having rating denominators greater than 10 \n",
    "3. then we will correct their denominators\n",
    "also "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> first we will correct numerator that have wrong rating (wrong data extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for wrong data : \n",
    "\n",
    "with pd.option_context('max_colwidth', 200):\n",
    "    display(twitter_archive_clean[twitter_archive_clean['text'].str.contains(r\"(\\d+\\.\\d*\\/\\d+)\")]\n",
    "            [['tweet_id', 'text', 'rating_numerator', 'rating_denominator']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting datatype to float to accept float values\n",
    "twitter_archive_clean[['rating_numerator', 'rating_denominator']] = twitter_archive_clean[['rating_numerator','rating_denominator']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating numerators manually: \n",
    "\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 883482846933004288), 'rating_numerator'] = 13.5\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 786709082849828864), 'rating_numerator'] = 9.75\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 778027034220126208), 'rating_numerator'] = 11.27\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 681340665377193984), 'rating_numerator'] = 9.5\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 680494726643068929), 'rating_numerator'] = 11.26\n",
    "\n",
    "# Testing code : \n",
    "with pd.option_context('max_colwidth', 200):\n",
    "    display(twitter_archive_clean[twitter_archive_clean['text'].str.contains(r\"(\\d+\\.\\d*\\/\\d+)\")]\n",
    "            [['tweet_id', 'text', 'rating_numerator', 'rating_denominator']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter_archive - Correc denominators\n",
    "\n",
    "a. manual cleaning of columns \n",
    "\n",
    "Five tweets with denominator not equal to 10 for special circunstances. Update both numerators and denominators when necessary. Delete other five tweets because they do not have actual ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking again before cleaning:\n",
    "print(df_twitter_archive['text'][784]) # Retweet -- will be deleted\n",
    "print(df_twitter_archive['text'][1068]) # Actual rating 14/10 manual change\n",
    "print(df_twitter_archive['text'][1662]) # Actual rating 10/10 manual change\n",
    "print(df_twitter_archive['text'][2335]) # Actual rating 9/10 manual change\n",
    "print(df_twitter_archive['text'][1663]) # Tweet to explain rating\n",
    "print(df_twitter_archive['text'][1635]) # 11 dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting its tweet id to clean better\n",
    "# not verifying from cleaning dataset because its not indexed properly \n",
    "print(df_twitter_archive['tweet_id'][1068])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: Update both numerators and denominators\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 740373189193256964), 'rating_numerator'] = 14\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 740373189193256964), 'rating_denominator'] = 10\n",
    "\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 682962037429899265), 'rating_numerator'] = 10\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 682962037429899265), 'rating_denominator'] = 10\n",
    "\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 666287406224695296), 'rating_numerator'] = 9\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 666287406224695296), 'rating_denominator'] = 10\n",
    "\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 722974582966214656), 'rating_numerator'] = 13\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 722974582966214656), 'rating_denominator'] = 10\n",
    "\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 716439118184652801), 'rating_numerator'] = 13.5\n",
    "twitter_archive_clean.loc[(twitter_archive_clean.tweet_id == 716439118184652801), 'rating_denominator'] = 10\n",
    "\n",
    "#CODE: Delete five tweets with no actual ratings\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 832088576586297345]\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 810984652412424192]\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 682808988178739200]\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 835246439529840640]\n",
    "twitter_archive_clean = twitter_archive_clean[twitter_archive_clean['tweet_id'] != 686035780142297088]\n",
    "\n",
    "#TEST: Left only the group dogs for programatically clean\n",
    "with pd.option_context('max_colwidth', 200):\n",
    "    display(twitter_archive_clean[twitter_archive_clean['rating_denominator'] != 10][['tweet_id',\n",
    "                                                                                      'text',\n",
    "                                                                                      'rating_numerator',\n",
    "                                                                                      'rating_denominator']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These tweets with denominator not equal to 10 are multiple dogs. For example, tweet_id 713900603437621000 has numerator and denominators 99/90 because there are 9 dogs in the picture https://t.co/mpvaVxKmc1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: Create a new column with rating in float type to avoid converting all int column to float\n",
    "twitter_archive_clean['rating'] = 10 * twitter_archive_clean['rating_numerator'] / twitter_archive_clean['rating_denominator'].astype(float)\n",
    "\n",
    "#TEST\n",
    "twitter_archive_clean.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning image_prediction\n",
    " df_image_prediction :\n",
    "        1. Drop 66 jpg_url duplicated\n",
    "        2. Create 1 column for image prediction and 1 column for confidence level\n",
    "        3. Delete columns that won't be used for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_prediction_clean = image_prediction_clean.drop_duplicates(subset=['jpg_url'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction_clean.jpg_url.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prediction_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Image_prediction - Create 1 column for image prediction and 1 column for confidence level\n",
    "\n",
    "Create a function where I keep the first true prediction along the confidence level as new columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE: the first true prediction (p1, p2 or p3) will be store in these lists\n",
    "dog_type = []\n",
    "confidence_list = []\n",
    "\n",
    "#create a function with nested if to capture the dog type and confidence level\n",
    "# from the first 'true' prediction\n",
    "def image(image_prediction_clean):\n",
    "    if image_prediction_clean['p1_dog'] == True:\n",
    "        dog_type.append(image_prediction_clean['p1'])\n",
    "        confidence_list.append(image_prediction_clean['p1_conf'])\n",
    "    elif image_prediction_clean['p2_dog'] == True:\n",
    "        dog_type.append(image_prediction_clean['p2'])\n",
    "        confidence_list.append(image_prediction_clean['p2_conf'])\n",
    "    elif image_prediction_clean['p3_dog'] == True:\n",
    "        dog_type.append(image_prediction_clean['p3'])\n",
    "        confidence_list.append(image_prediction_clean['p3_conf'])\n",
    "    else:\n",
    "        dog_type.append('Error')\n",
    "        confidence_list.append('Error')\n",
    "\n",
    "#series objects having index the image_prediction_clean column.        \n",
    "image_prediction_clean.apply(image, axis=1)\n",
    "\n",
    "#create new columns\n",
    "image_prediction_clean['dog_type'] = dog_type\n",
    "image_prediction_clean['confidence_list'] = confidence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows that has prediction_list 'error'\n",
    "image_prediction_clean = image_prediction_clean[image_prediction_clean['dog_type'] != 'Error']\n",
    "\n",
    "#TEST: \n",
    "image_prediction_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Delete columns that won't be used for analysis\n",
    "\n",
    "#CODE: print list of image_prediction columns\n",
    "print(list(image_prediction_clean))\n",
    "\n",
    "#Delete columns\n",
    "image_prediction_clean = image_prediction_clean.drop(['img_num', 'p1', \n",
    "                                                      'p1_conf', 'p1_dog', \n",
    "                                                      'p2', 'p2_conf', \n",
    "                                                      'p2_dog', 'p3', \n",
    "                                                      'p3_conf', \n",
    "                                                      'p3_dog'], 1)\n",
    "\n",
    "#TEST\n",
    "list(image_prediction_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet_json - keep 2174 original tweets\n",
    "\n",
    "#CODE:\n",
    "tweet_json_clean = tweet_json_clean[tweet_json_clean['retweeted_status'] == 'Original tweet']\n",
    "\n",
    "#TEST\n",
    "tweet_json_clean['retweeted_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidiness - Change tweet_id to type int64 in order to merge with the other 2 tables\n",
    "\n",
    "\n",
    "\n",
    "#CODE: change tweet_id from str to int\n",
    "tweet_json_clean['tweet_id'] = tweet_json_clean['tweet_id'].astype(int)\n",
    "\n",
    "#TEST\n",
    "tweet_json_clean['tweet_id'].dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidiness - All tables should be part of one dataset\n",
    "\n",
    "#CODE: create a new dataframe that merge twitter_archive_clean and \n",
    "#image_prediction_clean\n",
    "df_twitter1 = pd.merge(twitter_archive_clean, \n",
    "                      image_prediction_clean, \n",
    "                      how = 'left', on = ['tweet_id'])\n",
    "\n",
    "#keep rows that have picture (jpg_url)\n",
    "df_twitter1 = df_twitter1[df_twitter1['jpg_url'].notnull()]\n",
    "\n",
    "#TEST\n",
    "df_twitter1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = pd.merge(df_twitter1, tweet_json_clean, \n",
    "                      how = 'left', on = ['tweet_id'])\n",
    "\n",
    "#TEST\n",
    "df_twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter['rating_numerator'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the clean DataFrame in a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.to_csv('twitter_archive_master.csv', \n",
    "                 index=False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = pd.read_csv('twitter_archive_master.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights and visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter['dog_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dog_type = df_twitter.groupby('dog_type').filter(lambda x:len(x)>=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = df_dog_type['dog_type'].value_counts().index\n",
    "color = sns.color_palette()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df_dog_type,y='dog_type',order = order,color = color)\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel('Type of dog')\n",
    "plt.title('Histogram of the Most Rated Dog Type');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_twitter.groupby('dog_type').mean()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest rated dog type :  Bouvier_des_Flandres   \n",
    "### Lowest rated dog type :  Japanese_spaniel               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_twitter.groupby(['dog_type'])['rating'].mean()\n",
    "df = df.sort_values(ascending=False)\n",
    "df = df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar',color=color,figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_twitter.groupby(['dog_type'])['rating'].mean()\n",
    "df = df.sort_values(ascending=False)\n",
    "df = df[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar',color=color,figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common dog names :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelavent names.\n",
    "df_twitter.drop(df_twitter[df_twitter['name'] == 'a'].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.name.value_counts()[0:7].plot('barh', figsize=(10,8), title='Most Common Dog Names').set_xlabel(\"Number of Dogs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
